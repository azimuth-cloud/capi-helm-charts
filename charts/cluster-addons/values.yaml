---

# The name of the Cluster API cluster
# if not given, the release name is used
clusterName:

# Settings for the CNI addon
cni:
  # Indicates if a CNI should be deployed
  enabled: true
  # The CNI to deploy - supported values are calico or cilium
  type: calico
  # Settings for the calico CNI
  # See https://projectcalico.docs.tigera.io/getting-started/kubernetes/helm
  calico:
    chart:
      repo: https://projectcalico.docs.tigera.io/charts
      name: tigera-operator
      version: v3.31.3
    release:
      namespace: tigera-operator
      values:
        nodeSelector:
          node-role.kubernetes.io/control-plane: ""
    # Nova metadata service
    # See https://docs.openstack.org/nova/latest/user/metadata.html#the-metadata-service
    globalNetworkPolicy:
      denyNamespaceSelector: kubernetes.io/metadata.name != 'openstack-system'
      allowPriority: 20
      denyPriority: 10
      allowEgressCidrs:
        - "0.0.0.0/0"
      denyEgressCidrs:
        - "169.254.169.254/32"
      allowv6EgressCidrs:
        - "::/0"
      denyv6EgressCidrs:
        - "fe80::a9fe:a9fe/128"
  # Settings for the Cilium CNI
  # See https://docs.cilium.io/en/stable/gettingstarted/k8s-install-helm/ for details
  cilium:
    chart:
      repo: https://helm.cilium.io/
      name: cilium
      version: 1.19.0
    release:
      namespace: kube-system
      values: {}

# Settings for CSI addons
csi:
  # Settings for the CephFS CSI
  cephfs:
    enabled: false
    chart:
      repo: https://ceph.github.io/csi-charts
      name: ceph-csi-cephfs
      version: 3.11.0
    release:
      namespace: csi-ceph-system
      values: {}

# Settings for the OpenStack integrations
openstack:
  # Indicates if the OpenStack integrations should be enabled
  enabled: false
  # The target namespace for the OpenStack integrations
  targetNamespace: openstack-system
  # cloud-config options for the OpenStack integrations
  # The [Global] section is configured to use the target cloud
  # See https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/using-openstack-cloud-controller-manager.md#config-openstack-cloud-controller-manager
  # and https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md#block-storage
  cloudConfig:
    # By default, ignore volume AZs for Cinder as most clouds have a single globally-attachable Cinder AZ
    BlockStorage:
      ignore-volume-az: true
  # Settings for the Cloud Controller Manager (CCM)
  ccm:
    # Indicates if the OpenStack CCM should be enabled
    # By default, the CCM is enabled if the OpenStack integrations are enabled
    # See https://github.com/kubernetes/cloud-provider-openstack/blob/master/charts/openstack-cloud-controller-manager/values.yaml
    enabled: true
    chart:
      repo: https://kubernetes.github.io/cloud-provider-openstack
      name: openstack-cloud-controller-manager
      version: 2.32.0
    values: {}
  # Settings for the Cinder CSI plugin
  csiCinder:
    # Indicates if the Cinder CSI should be enabled
    # By default, it is enabled if the OpenStack integrations are enabled
    # See https://github.com/kubernetes/cloud-provider-openstack/blob/master/charts/cinder-csi-plugin/values.yaml
    enabled: true
    chart:
      repo: https://kubernetes.github.io/cloud-provider-openstack
      name: openstack-cinder-csi
      version: 2.32.0
    values:
      csi:
        plugin:
          controllerPlugin:
            nodeSelector:
              node-role.kubernetes.io/control-plane: ""
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                effect: NoSchedule
    # Definition of the default storage class for Cinder CSI
    defaultStorageClass:
      # Indicates if the storage class should be enabled
      enabled: true
      # The name of the storage class
      name: csi-cinder
      # Indicates if the Cinder default storage class is the cluster default storage class
      isClusterDefault: true
      # The reclaim policy for the storage class
      reclaimPolicy: Delete
      # Indicates if volume expansion is allowed
      allowVolumeExpansion: true
      # Controls when volume binding and dynamic provisioning should occur
      volumeBindingMode: WaitForFirstConsumer
      # The allowed topologies for the storage class
      allowedTopologies:
      # Filesystem type to use for volumes provisioned with the storage class
      # If not given, the default filesystem type will be used
      fstype:
      # The Cinder availability zone to use for volumes provisioned by the storage class
      availabilityZone: nova
      # The Cinder volume type to use for volumes provisioned by the storage class
      # If not given, the default volume type will be used
      volumeType:
    # Additional storage classes to create for the Cinder CSI
    # For each item, the properties from the default storage class are supported (except enabled and isClusterDefault)
    additionalStorageClasses: []
  # Settings for the Manila CSI plugin
  csiManila:
    # Indicates if the Manila CSI should be enabled
    # By default, it is disabled as Manila is not commonly available
    # See https://github.com/kubernetes/cloud-provider-openstack/blob/master/charts/manila-csi-plugin/values.yaml
    enabled: false
    chart:
      repo: https://kubernetes.github.io/cloud-provider-openstack
      name: openstack-manila-csi
      version: 2.30.0
    values: {}
    # Definition of the default storage class for the Manila CSI
    defaultStorageClass:
      # Indicates if the storage class should be enabled
      enabled: true
      # The name of the storage class
      name: csi-manila
      # Indicates if the Manila default storage class is the cluster default storage class
      isClusterDefault: false
      # The provisioner for the storage class
      # If not given and the Ceph CSI plugin is installed, cephfs.manila.csi.openstack.org is used
      provisioner:
      # The reclaim policy for the storage class
      reclaimPolicy: Delete
      # Indicates if volume expansion is allowed
      allowVolumeExpansion: true
      # Controls when volume binding and dynamic provisioning should occur
      volumeBindingMode: WaitForFirstConsumer
      # The allowed topologies for the storage class
      allowedTopologies:
      # The parameters for the storage class
      # See https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/manila-csi-plugin/using-manila-csi-plugin.md#controller-service-volume-parameters
      parameters:
        # The Manila share type to use
        # If not given and the Ceph CSI plugin is installed, cephfs is used
        type:
    # Additional storage classes to create for the Manila CSI
    # For each item, the properties from the default storage class are supported (except for "enabled")
    additionalStorageClasses: []
  k8sKeystoneAuth:
    enabled: false
    targetNamespace: kube-system
    chart:
      repo: https://catalyst-cloud.github.io/capi-plugin-helm-charts
      name: k8s-keystone-auth
      version: 1.4.0

# Setting for Reloader used to restart workloads on Secret/ConfigMap changes
reloader:
  # Indicates if Reloader should be enabled
  enabled: false
  chart:
    repo: https://stakater.github.io/stakater-charts
    name: reloader
    version: 2.2.7
  release:
    # Deploy to the same namespace as OpenStack services
    namespace: openstack-system
    values:
      reloader:
        # Restrict scope to namespace
        watchGlobally: false

# Settings for etcd defragmentation jobs
etcdDefrag:
  # Indicates if the etcd defragmentation job should be enabled
  enabled: true
  chart:
    repo: https://azimuth-cloud.github.io/capi-helm-charts
    name: etcd-defrag
    version:  # Defaults to the same version as this chart
  release:
    # This should be namespace in which the etcd pods are deployed
    namespace: kube-system
    values: {}

# Settings for the metrics server
# https://github.com/kubernetes-sigs/metrics-server#helm-chart
metricsServer:
  # Indicates if the metrics server should be deployed
  enabled: true
  chart:
    repo: https://kubernetes-sigs.github.io/metrics-server
    name: metrics-server
    version: 3.12.2
  release:
    namespace: kube-system
    values: {}

# Settings for the Kubernetes dashboard
# https://github.com/kubernetes-retired/dashboard/tree/master/charts/kubernetes-dashboard
kubernetesDashboard:
  # Indicates if the Kubernetes dashboard should be enabled
  enabled: false
  chart:
    repo: https://kubernetes-retired.github.io/dashboard
    name: kubernetes-dashboard
    version: 6.0.8
  release:
    namespace: kubernetes-dashboard
    values: {}

# Settings for ingress controllers
ingress:
  # Indicates if ingress controllers should be enabled
  enabled: false
  # Settings for the Nginx ingress controller
  # https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx#configuration
  nginx:
    # Indicates if the Nginx ingress controller should be enabled
    # The Nginx ingress controller is enabled by default if ingress controllers are enabled
    enabled: true
    chart:
      repo: https://kubernetes.github.io/ingress-nginx
      name: ingress-nginx
      version: 4.14.3
    release:
      namespace: ingress-nginx
      values: {}

# Settings for cluster monitoring
monitoring:
  # Indicates if the cluster monitoring should be enabled
  enabled: false
  # Config for the kube-prometheus-stack helm chart
  # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack
  kubePrometheusStack:
    chart:
      repo: https://prometheus-community.github.io/helm-charts
      name: kube-prometheus-stack
      version: 80.14.4
    release:
      namespace: monitoring-system
      values:
        # Enable persistence by default for prometheus and alertmanager
        alertmanager:
          alertmanagerSpec:
            # By default, retain 7 days of data
            retention: 168h
            storage:
              volumeClaimTemplate:
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
        prometheus:
          prometheusSpec:
            # The amount of data that is retained will be 90 days or 95% of the size of the
            # persistent volume, whichever is reached first
            retention: 90d
            storageSpec:
              volumeClaimTemplate:
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
  lokiStack:
    enabled: true
    chart:
      repo: https://grafana.github.io/helm-charts
      name: loki-stack
      version: 2.10.2
    release:
      namespace: monitoring-system
      values:
        loki:
          # Enable retention and configure a default retention period of 31 days
          config:
            compactor:
              retention_enabled: true
            limits_config:
              retention_period: 72h
          # Enable persistence by default
          persistence:
            enabled: true
            size: 10Gi

  promtail:
    enabled: false
    chart:
      repo: https://grafana.github.io/helm-charts
      name: promtail
      version: 6.17.0
    release:
      namespace: monitoring-system
      values: {}

  alloy:
    enabled: false
    chart:
      repo: https://grafana.github.io/helm-charts
      name: alloy
      version: 1.5.2
    release:
      namespace: monitoring-system
      values:
        crds:
          create: false
        controller:
          type: 'daemonset'
          tolerations:
            - operator: Exists

  # Note(sd109): Loki v2 is the successor to the lokiStack addon since the Helm
  # chart for the latter is no longer maintained upstream.

  # No backwards compatibility promises are made between the old lokiStack addon and
  # the newer Loki v2 addon. For existing clusters with lokiStack enabled, switching
  # to Loki v2 with the default config values will result in the storage volume
  # containing historical logging data being deleted. A new volume will be
  # provisioned in its place for the new Loki v2 addon.

  # For use cases requiring historical log retention, it is the chart consumers's
  # responsibility to research, test and implement a viable upgrade path which works
  # for them. Covering all possible upgrade paths with default chart config is not
  # possible. As a starting point for plotting an upgrade path, the comments
  # surrounding the various chart values below attempt to point users in the right
  # direction based on some initial research done while implementing the Loki v2
  # addon. This config is only offered as a guide only and does not on it's own
  # achieve full historical log retention during migration between lokiStack to Loki
  # v2 addons.
  loki_v2:
    enabled: false
    chart:
      # The chart repository that contains the chart to use
      repo: https://grafana.github.io/helm-charts
      # The name of the chart to use
      name: loki
      # The version of the chart to use (must be an exact version)
      version: 5.48.0
    # The Helm values to use for the release
    release:
      namespace: monitoring-system
      values:
        # In case historical logs are required, configure fullnameOverride to
        # ensure that resource names match the old loki-stack
        # HelmRelease so that it correctly re-adopts the PVC
        # fullnameOverride: loki-stack
        loki:
          compactor:
            retention_enabled: true
          limits_config:
            retention_period: 72h
          auth_enabled: false
          commonConfig:
            replication_factor: 1
            # If historical logs are required, configure Loki to use same
            # path inside the data PVC as old loki-stack.
            # path_prefix: /var/loki/loki
          storage:
            type: 'filesystem'
            # If historical logs are required, configure Loki to
            # store data in /var/loki to match loki-stack
            # filesystem:
            #   chunks_directory: /var/loki/loki/chunks
            #   rules_directory: /var/loki/loki/rules
          # If historical logs are required, additional schema configuration
          # is needed to allow loki to ingest old loki-stack logging data.
          # For example:
          # schemaConfig:
          #   configs:
          #       # The "from" date marks from when the old data was written
          #     - from: "2020-10-24"
          #       index:
          #         period: 24h
          #         prefix: index_
          #       object_store: filesystem
          #       schema: v11
          #       store: boltdb-shipper
          #       # The "from" date marks from when the new data is written
          #     - from: "2025-10-17"
          #       index:
          #         period: 24h
          #         prefix: loki_index_
          #       object_store: filesystem
          #       schema: v12
          #       store: boltdb-shipper
        singleBinary:
          replicas: 1
          # Enable persistence by default
          persistence:
            enabled: true
            size: 10Gi
            # Required to allow resizing of PVC during Helm upgrades
            enableStatefulSetAutoDeletePVC: false

  loki_v3:
    enabled: false
    chart:
      # The chart repository that contains the chart to use
      repo: https://grafana.github.io/helm-charts
      # The name of the chart to use
      name: loki
      # The version of the chart to use (must be an exact version)
      version: 6.52.0
    # The Helm values to use for the release
    release:
      namespace: monitoring-system
      values:
        deploymentMode: SingleBinary
        backend:
          replicas: 0
        read:
          replicas: 0
        write:
          replicas: 0
        ingester:
          replicas: 0
        querier:
          replicas: 0
        queryFrontend:
          replicas: 0
        queryScheduler:
          replicas: 0
        distributor:
          replicas: 0
        compactor:
          replicas: 0
        indexGateway:
          replicas: 0
        bloomCompactor:
          replicas: 0
        bloomGateway:
          replicas: 0
        # In case historical logs are required, configure fullnameOverride to
        # ensure that resource names match the old loki-stack
        # HelmRelease so that it correctly re-adopts the PVC
        # fullnameOverride: loki-stack
        loki:
          schemaConfig:
            configs:
              - from: "2024-04-01"
                store: tsdb
                object_store: filesystem
                schema: v13
                index:
                  prefix: loki_index_
                  period: 24h
          storage:
            type: 'filesystem'
          compactor:
            retention_enabled: true
            delete_request_store: filesystem
          auth_enabled: false
          commonConfig:
            replication_factor: 1
          limits_config:
            allow_structured_metadata: true
            volume_enabled: true
            retention_period: 72h
          ruler:
            enable_api: true
            # If historical logs are required, configure Loki to use same
            # path inside the data PVC as old loki-stack.
           # path_prefix: /var/loki/loki-v3
          # storage:
          #   type: 'filesystem'
            # If historical logs are required, configure Loki to
            # store data in /var/loki to match loki-stack
            # filesystem:
            #   chunks_directory: /var/loki/loki-v3/chunks
            #   rules_directory: /var/loki/loki-v3/rules
          # If historical logs are required, additional schema configuration
          # is needed to allow loki to ingest old loki-stack logging data.
          # For example:
          # schemaConfig:
          #   configs:
          #       # The "from" date marks from when the old data was written
          #     - from: "2020-10-24"
          #       index:
          #         period: 24h
          #         prefix: index_
          #       object_store: filesystem
          #       schema: v11
          #       store: boltdb-shipper
          #     - from: "2026-01-19"
          #       object_store: s3
          #       store: tsdb
          #       schema: v13
          #       index:
          #         prefix: index_
          #         period: 24h
              #   # The "from" date marks from when the new data is written
              # - from: "2025-10-17"
              #   index:
              #     period: 24h
              #     prefix: loki_index_
              #   object_store: filesystem
              #   schema: v12
              #   store: boltdb-shipper
        singleBinary:
          replicas: 1
          # Enable persistence by default
          persistence:
            enabled: true
            size: 10Gi
            # Required to allow resizing of PVC during Helm upgrades
            enableStatefulSetAutoDeletePVC: false

  # Configuration for the blackbox exporter
  blackboxExporter:
    enabled: true
    chart:
      repo: https://prometheus-community.github.io/helm-charts
      name: prometheus-blackbox-exporter
      version: 11.8.0
    release:
      namespace: monitoring-system
      values: {}
        # Example of adding additional scrape targets
        # serviceMonitor:
        #   targets:
        #     - name: example
        #       url: http://example.com/healthz

# Settings for node feature discovery
# https://github.com/kubernetes-sigs/node-feature-discovery/tree/master/deployment/helm/node-feature-discovery
nodeFeatureDiscovery:
  # Indicates if node feature discovery should be enabled
  enabled: true
  chart:
    repo: https://kubernetes-sigs.github.io/node-feature-discovery/charts
    name: node-feature-discovery
    version: 0.17.3
  release:
    namespace: node-feature-discovery
    values: {}

# Settings for the NVIDIA GPU operator
nvidiaGPUOperator:
  # Indicates if the NVIDIA GPU operator should be enabled
  # Note that because it uses node feature discovery to run only on nodes
  # with an NVIDIA GPU available, the overhead of enabling this on clusters
  # that do not need it now but may need it in the future is low
  enabled: true
  chart:
    repo: https://helm.ngc.nvidia.com/nvidia
    name: gpu-operator
    version: v25.10.1
  release:
    namespace: gpu-operator
    values:
      dcgmExporter:
        serviceMonitor:
          enabled: true

intelDevicePlugin:
  enabled: false
  operator:
    chart:
      repo: https://intel.github.io/helm-charts
      name: intel-device-plugins-operator
      version: 0.32.0
    release:
      namespace: intel
      values:
        manager:
          devices:
            gpu: true
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 160Mi
  gpuPlugin:
    chart:
      repo: https://intel.github.io/helm-charts
      name: intel-device-plugins-gpu
      version: 0.32.0
    release:
      namespace: intel
      values:
        sharedDevNum: 1
        preferredAllocationPolicy: none
        logLevel: 4
        enableMonitoring: true
        tolerations:
          - key: "gpu.intel.com/i915"
            operator: "Exists"
            effect: "NoSchedule"
  xpuManagerMonitoring:
    enabled: true
    namespace: intel
    repo:
      url: https://github.com/intel/xpumanager
      tag: V1.2.39

certManager:
  enabled: false
  chart:
    repo: https://charts.jetstack.io
    name: cert-manager
    version: v1.17.0
  release:
    namespace: cert-manager
    values:
      crds:
        enabled: true

# Settings for the Mellanox network operator
mellanoxNetworkOperator:
  # Indicates if the network operator should be enabled
  # Note that because it uses node feature discovery to run only on nodes
  # with a Mellanox NIC available, the overhead of enabling this on clusters
  # that do not need it now but may need it in the future is low
  enabled: true
  chart:
    repo: https://helm.ngc.nvidia.com/nvidia
    name: network-operator
    version: 25.1.0
  release:
    namespace: network-operator
    values: {}

# Settings for the node problem detector
nodeProblemDetector:
  # Indicates if the node problem detector should be enabled
  enabled: true
  chart:
    repo: https://charts.deliveryhero.io
    name: node-problem-detector
    version: 2.3.14
  release:
    namespace: node-problem-detector
    values: {}

# Settings for any custom addons
custom: {}
  # # Indexed by the name of the release on the target cluster
  # my-custom-helm-release:
  #   # Indicates that this is a Helm addon
  #   kind: HelmRelease
  #   spec:
  #     # The namespace that the addon should be in
  #     namespace: my-namespace
  #     # Details of the Helm chart to use
  #     chart:
  #       # The chart repository that contains the chart to use
  #       repo: https://my-project/charts
  #       # The name of the chart to use
  #       name: my-chart
  #       # The version of the chart to use (must be an exact version)
  #       version: 1.5.0
  #     # The Helm values to use for the release
  #     values: {}
  # # Indexed by the name of the release on the target cluster
  # my-custom-manifests:
  #   # Indicates that this is a Manifests addon
  #   kind: Manifests
  #   spec:
  #     # The namespace that the addon should be in
  #     namespace: my-namespace
  #     # The manifests for the addon, indexed by filename
  #     manifests:
  #       secret.yaml: |-
  #         apiVersion: v1
  #         kind: Secret
  #         metadata:
  #           name: my-secret
  #         stringData:
  #           secret-file: "secret-data"
